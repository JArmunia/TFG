{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy \n",
    "import util\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import math\n",
    "import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA = \"E:Corrected_FA/ALL_DATA/\"\n",
    "info_data = \"idaSearch_8_01_2020.csv\"\n",
    "\n",
    "# Obtenemos los diccionarios con los nombres de los ficheros que contienen las imágenes\n",
    "AD_CN, groups = util.obtain_data_files(ALL_DATA, info_data)\n",
    "\n",
    "# Cargamos las imágenes\n",
    "CN_imgs = np.array(util.load_data(ALL_DATA, AD_CN[\"CN\"]), dtype='float32')\n",
    "\n",
    "AD_imgs = util.load_data(ALL_DATA, AD_CN[\"AD\"])\n",
    "\n",
    "# Extendemos la clase con menos ejemplos\n",
    "AD_imgs = np.array(util.extend_class(AD_imgs, len(CN_imgs)), dtype='float32')\n",
    "\n",
    "# Creamos las etiquetas 1: AD, 0:CN\n",
    "CN_labels = np.zeros((len(CN_imgs),1), dtype = \"int32\")\n",
    "AD_labels = np.ones((len(AD_imgs),1), dtype = \"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para crear el modelo seleccionando número de filtros y neuronas, dropout rate y parámetro de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters, neurons, dropout_rate, regularization):\n",
    "    layers = tf.keras.layers\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv3D(filters, 11, strides = (4,4,4), padding= 'valid', input_shape=(91,109,91, 1), activation = 'relu'),\n",
    "        layers.BatchNormalization(),     \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 5, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),    \n",
    "        layers.GlobalAveragePooling3D(),\n",
    "\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para seleccionar únicamente dropout rate y regularización.\n",
    "\n",
    "Esta es la función que utilicé finalmente ya que utilizamos el número de filtros y neuronas de VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_def(dropout, regularization):\n",
    "    layers = tf.keras.layers\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv3D(64, 11, strides = (4,4,4), padding= 'valid', input_shape=(91,109,91, 1)),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),       \n",
    "\n",
    "        layers.Conv3D(128, 5, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv3D(256, 3, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv3D(512, 3, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling3D(),\n",
    "\n",
    "        layers.Conv3D(512, 3, strides = (1,1,1), padding= 'valid'),    \n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.GlobalAveragePooling3D(),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(1, activation = 'sigmoid')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para probar un modelo\n",
    "Modelo dado número de filtros y neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(train_ds, val_ds, train_size, filters, neurons, batch_size = 32, dropout=0.05, reg=0.003, learning_rate = 3e-7, fold = 0, n_epoch = 200):\n",
    "    \"\"\" Crea, compila y entrena un modelo con los parámetros obtenidos, además guarda el modelo con mejor loss y el modelo con mejor accuracy.\n",
    "    Devuelve la evaluación del modelo con mejor loss y la del modelo con mejor accuracy, los path de los modelos y el history del entrenamiento\"\"\"\n",
    "    \n",
    "    loss_path = \"model_loss_{}_{}_{}_{}_{}.h5\".format(filters,neurons, dropout, reg, fold)\n",
    "    \n",
    "    checkpoint_cb_loss = keras.callbacks.ModelCheckpoint(loss_path, monitor=\"val_loss\", save_best_only = True) \n",
    "    \n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs_cv\") \n",
    "    def get_run_logdir(): \n",
    "        run_id = \"run_{}_{}_{}_{}_{}\".format(filters,neurons, dropout, reg, fold) \n",
    "        return os.path.join(root_logdir, run_id) \n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "    # Se crea el modelo\n",
    "    m = create_model_def(dropout, reg)\n",
    "    # Se compila\n",
    "    m.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])       \n",
    "    # Se entrena\n",
    "    history = m.fit(train_ds.repeat(), epochs = n_epoch, steps_per_epoch= train_size/batch_size, \n",
    "                    validation_data = val_ds, verbose = 0, callbacks =[checkpoint_cb_loss, \n",
    "                                                                       tensorboard_cb]) \n",
    "    # Evaluacion del modelo con mejor loss\n",
    "    m = keras.models.load_model(loss_path) \n",
    "    evaluation_loss = m.evaluate(val_ds)\n",
    "    \n",
    "    \n",
    "    return {\"ev_loss\": evaluation_loss, \"loss_path\": loss_path, \"history\": history}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo dado dropout rate y regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model_final(train_ds, val_ds, train_size, batch_size = 32, dropout=0.05, reg=0.003, learning_rate = 3e-7, fold = 0, n_epoch = 200):\n",
    "    \"\"\" Crea, compila y entrena un modelo con los parámetros obtenidos, además guarda el modelo con mejor loss y el modelo con mejor accuracy.\n",
    "    Devuelve la evaluación del modelo con mejor loss y la del modelo con mejor accuracy, los path de los modelos y el history del entrenamiento\"\"\"\n",
    "    \n",
    "    loss_path = \"model_loss_{}_{}_{}.h5\".format(dropout, reg, fold)\n",
    "    \n",
    "    checkpoint_cb_loss = keras.callbacks.ModelCheckpoint(loss_path, monitor=\"val_loss\", save_best_only = True) \n",
    "    \n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs_cv\") \n",
    "    def get_run_logdir(): \n",
    "        run_id = \"run__{}_{}_{}\".format(dropout, reg, fold) \n",
    "        return os.path.join(root_logdir, run_id) \n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "    # Se crea el modelo\n",
    "    m = create_model_def(dropout, reg)\n",
    "    # Se compila\n",
    "    m.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])       \n",
    "    # Se entrena\n",
    "    history = m.fit(train_ds.repeat(), epochs = n_epoch, steps_per_epoch= train_size/batch_size, \n",
    "                    validation_data = val_ds, verbose = 0, callbacks =[checkpoint_cb_loss, \n",
    "                                                                       tensorboard_cb]) \n",
    "    # Evaluacion del modelo con mejor loss\n",
    "    m = keras.models.load_model(loss_path) \n",
    "    evaluation_loss = m.evaluate(val_ds)\n",
    "    \n",
    "    \n",
    "    return {\"ev_loss\": evaluation_loss, \"loss_path\": loss_path, \"history\": history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data = util.train_test_split(CN_imgs, CN_labels, AD_imgs, AD_labels, 0.15)\n",
    "\n",
    "CN_imgs, AD_imgs = None, None # Liberamos memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0cd579a0d98c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_size\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mevaluations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtry_model_final\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mevaluation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ev_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"history\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-af3c40b9231c>\u001b[0m in \u001b[0;36mtry_model_final\u001b[1;34m(train_ds, val_ds, train_size, batch_size, dropout, reg, learning_rate, fold, n_epoch)\u001b[0m\n\u001b[0;32m     21\u001b[0m     history = m.fit(train_ds.repeat(), epochs = n_epoch, steps_per_epoch= train_size/batch_size, \n\u001b[0;32m     22\u001b[0m                     validation_data = val_ds, verbose = 0, callbacks =[checkpoint_cb_loss, \n\u001b[1;32m---> 23\u001b[1;33m                                                                        tensorboard_cb]) \n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Evaluacion del modelo con mejor loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m                   mode=ModeKeys.TEST):\n\u001b[0;32m    382\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0meval_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m                   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m                   eval_result = run_one_epoch(\n\u001b[0;32m    385\u001b[0m                       \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \"\"\"\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3321\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3322\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3323\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3324\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3325\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    819\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 821\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    822\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfg\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    140\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m    141\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[0;32m    143\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dropouts = [0.2, .3,.4,.5]\n",
    "n_epoch = 250\n",
    "batch_size = 32\n",
    "n_folds = 5\n",
    "effective_folds = 3\n",
    "histories = []\n",
    "\n",
    "best_model_loss = \"\"\n",
    "best_loss = np.inf\n",
    "best_acc = 0\n",
    "dropout = None\n",
    "reg = 0.003\n",
    "lr = 1e-6\n",
    "\n",
    "\n",
    "best_loss_parameters = []\n",
    "\n",
    "for d in dropouts:    \n",
    "    run_evaluations = []\n",
    "    for fold in range(effective_folds):\n",
    "        start = time.time()\n",
    "        #print(\"Iniciado modelo con f = {} y n = {}\".format(f,n))\n",
    "\n",
    "        fold_data = util.k_fold(data[\"train_imgs\"], data[\"train_labels\"], n_folds, fold )\n",
    "        train_ds = fold_data[\"train_ds\"].map(lambda tensor, labels : util.transform(tensor,labels), num_parallel_calls=16)\\\n",
    "                                        .batch(batch_size).prefetch(8)\n",
    "        val_ds = fold_data[\"val_ds\"].batch(fold_data[\"val_size\"])\n",
    "        train_size = fold_data[\"train_size\"]\n",
    "\n",
    "        evaluations = try_model_final(train_ds, val_ds, train_size, dropout = d, fold= fold, learning_rate = lr, n_epoch = n_epoch)\n",
    "        evaluation_loss = evaluations[\"ev_loss\"]\n",
    "        history = evaluations[\"history\"]\n",
    "\n",
    "        print(\"{} dropout, {} regularization, {} fold\".format(d, reg, fold))\n",
    "        print(\"Loss: {}, Accuracy: {}\".format(evaluation_loss[0], evaluation_loss[1]))\n",
    "\n",
    "        histories.append(history)\n",
    "        run_evaluations.append(evaluation_loss)\n",
    "        end = time.time()\n",
    "        print(\"Tiempo de ejecucion de fold: {}\".format(end-start))\n",
    "\n",
    "    # Comprobamos si el modelo con mejor loss es el mejor hasta el momento\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for i in range(len(run_evaluations)):\n",
    "        loss += run_evaluations[i][0]\n",
    "        acc += run_evaluations[i][1]\n",
    "    loss /= len(run_evaluations)\n",
    "    acc /= len(run_evaluations)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_loss = evaluations[\"loss_path\"]\n",
    "        print(\"Nuevo mejor modelo de loss con {}\".format(best_loss))\n",
    "\n",
    "    if acc > best_acc:\n",
    "\n",
    "        best_acc = acc\n",
    "        best_model_acc = evaluations[\"loss_path\"]\n",
    "        print(\"Nuevo mejor modelo de accuracy con {}\".format(best_acc))\n",
    "\n",
    "    print(\"Definitive evaluation of best loss of model with {} dropout, {} regularization, {} fold\".format(dropout, reg, fold))\n",
    "    print(\"Loss: {}, Accuracy: {}\".format(loss, acc))\n",
    "\n",
    "        \n",
    "print(\"Terminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
