{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import keras\n",
    "import util\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics  import accuracy_score\n",
    "import math\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los nombres de las im치genes y sus etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADNI_003_S_4081_MR_corrected_FA_image_Br_20120...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADNI_003_S_4081_MR_corrected_FA_image_Br_20131...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADNI_003_S_4119_MR_corrected_FA_image_Br_20120...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADNI_003_S_4119_MR_corrected_FA_image_Br_20120...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADNI_003_S_4119_MR_corrected_FA_image_Br_20130...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image Label\n",
       "0  ADNI_003_S_4081_MR_corrected_FA_image_Br_20120...    CN\n",
       "1  ADNI_003_S_4081_MR_corrected_FA_image_Br_20131...    CN\n",
       "2  ADNI_003_S_4119_MR_corrected_FA_image_Br_20120...    CN\n",
       "3  ADNI_003_S_4119_MR_corrected_FA_image_Br_20120...    CN\n",
       "4  ADNI_003_S_4119_MR_corrected_FA_image_Br_20130...    CN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_DATA = \"E:Corrected_FA/ALL_DATA/\"\n",
    "info_data = \"idaSearch_8_01_2020.csv\"\n",
    "\n",
    "# Obtenemos los diccionarios con los nombres de los ficheros que contienen las im치genes\n",
    "AD_CN, groups = util.obtain_data_files(ALL_DATA, info_data)\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for key in AD_CN.keys():\n",
    "    for img in AD_CN[key]:\n",
    "        imgs.append(img) \n",
    "        labels.append(key)\n",
    "\n",
    "image_labels = pd.DataFrame({\"Image\": imgs, \"Label\": labels} )\n",
    "image_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset de im치genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpcas_temporal_right = pd.read_csv(\"FPCAS_Ventricle_Prueba_Norm.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos im치genes con etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPC_1</th>\n",
       "      <th>FPC_2</th>\n",
       "      <th>FPC_3</th>\n",
       "      <th>FPC_4</th>\n",
       "      <th>FPC_5</th>\n",
       "      <th>FPC_6</th>\n",
       "      <th>FPC_7</th>\n",
       "      <th>FPC_8</th>\n",
       "      <th>FPC_9</th>\n",
       "      <th>FPC_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FPC_39</th>\n",
       "      <th>FPC_40</th>\n",
       "      <th>FPC_41</th>\n",
       "      <th>FPC_42</th>\n",
       "      <th>FPC_43</th>\n",
       "      <th>FPC_44</th>\n",
       "      <th>FPC_45</th>\n",
       "      <th>FPC_46</th>\n",
       "      <th>FPC_47</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.426598</td>\n",
       "      <td>0.112181</td>\n",
       "      <td>-0.322595</td>\n",
       "      <td>-0.033974</td>\n",
       "      <td>0.137037</td>\n",
       "      <td>-0.154825</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>-0.041808</td>\n",
       "      <td>0.032636</td>\n",
       "      <td>-0.120359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078227</td>\n",
       "      <td>-0.115859</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.071014</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>-0.071799</td>\n",
       "      <td>-0.013446</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>-0.033078</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.447385</td>\n",
       "      <td>0.120397</td>\n",
       "      <td>-0.326797</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>0.071308</td>\n",
       "      <td>-0.121966</td>\n",
       "      <td>0.084857</td>\n",
       "      <td>-0.073673</td>\n",
       "      <td>0.041247</td>\n",
       "      <td>-0.115501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038899</td>\n",
       "      <td>-0.087622</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.033449</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>-0.012131</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>-0.051622</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.671375</td>\n",
       "      <td>0.115622</td>\n",
       "      <td>0.097130</td>\n",
       "      <td>0.073061</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.046433</td>\n",
       "      <td>0.031110</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>0.023665</td>\n",
       "      <td>-0.045756</td>\n",
       "      <td>-0.038222</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.635482</td>\n",
       "      <td>0.068551</td>\n",
       "      <td>0.149279</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>-0.014603</td>\n",
       "      <td>0.046951</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.003189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005380</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>-0.070958</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.062745</td>\n",
       "      <td>0.015311</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.531853</td>\n",
       "      <td>-0.371275</td>\n",
       "      <td>0.160936</td>\n",
       "      <td>-0.147053</td>\n",
       "      <td>0.181173</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>-0.113544</td>\n",
       "      <td>0.143626</td>\n",
       "      <td>0.046215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017159</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>-0.004663</td>\n",
       "      <td>-0.079088</td>\n",
       "      <td>-0.082042</td>\n",
       "      <td>-0.019592</td>\n",
       "      <td>0.076686</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1.512948</td>\n",
       "      <td>-0.373168</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>-0.007119</td>\n",
       "      <td>-0.110302</td>\n",
       "      <td>-0.001779</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>-0.120261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>-0.079238</td>\n",
       "      <td>-0.099450</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.015750</td>\n",
       "      <td>0.043629</td>\n",
       "      <td>-0.051654</td>\n",
       "      <td>-0.040108</td>\n",
       "      <td>0.055429</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1.537276</td>\n",
       "      <td>0.165208</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>-0.045060</td>\n",
       "      <td>-0.034294</td>\n",
       "      <td>0.063803</td>\n",
       "      <td>-0.100678</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>-0.031374</td>\n",
       "      <td>0.052864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074753</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>-0.052607</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>-0.056600</td>\n",
       "      <td>0.107450</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1.403889</td>\n",
       "      <td>0.080721</td>\n",
       "      <td>-0.193643</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>-0.048856</td>\n",
       "      <td>0.132803</td>\n",
       "      <td>0.133546</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.169236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009194</td>\n",
       "      <td>-0.029963</td>\n",
       "      <td>-0.062132</td>\n",
       "      <td>-0.039073</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>-0.050268</td>\n",
       "      <td>0.074394</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1.549337</td>\n",
       "      <td>0.191282</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>0.054031</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.025229</td>\n",
       "      <td>0.044374</td>\n",
       "      <td>0.039591</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054471</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>-0.050911</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.041045</td>\n",
       "      <td>-0.016493</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1.536159</td>\n",
       "      <td>0.230381</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>-0.066380</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>-0.008271</td>\n",
       "      <td>-0.019923</td>\n",
       "      <td>0.070466</td>\n",
       "      <td>0.091987</td>\n",
       "      <td>0.070810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>-0.058377</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows 칑 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FPC_1     FPC_2     FPC_3     FPC_4     FPC_5     FPC_6     FPC_7  \\\n",
       "0    1.426598  0.112181 -0.322595 -0.033974  0.137037 -0.154825  0.014088   \n",
       "1    1.447385  0.120397 -0.326797 -0.004794  0.071308 -0.121966  0.084857   \n",
       "2    1.671375  0.115622  0.097130  0.073061 -0.007771  0.018225  0.117936   \n",
       "3    1.635482  0.068551  0.149279  0.064166 -0.014603  0.046951  0.025481   \n",
       "4    1.531853 -0.371275  0.160936 -0.147053  0.181173  0.045500  0.081013   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "365  1.512948 -0.373168  0.136220 -0.007119 -0.110302 -0.001779 -0.073120   \n",
       "366  1.537276  0.165208  0.091943 -0.045060 -0.034294  0.063803 -0.100678   \n",
       "367  1.403889  0.080721 -0.193643  0.015488 -0.048856  0.132803  0.133546   \n",
       "368  1.549337  0.191282  0.039588  0.054031  0.033168  0.025229  0.044374   \n",
       "369  1.536159  0.230381  0.087843 -0.066380 -0.010315 -0.008271 -0.019923   \n",
       "\n",
       "        FPC_8     FPC_9    FPC_10  ...    FPC_39    FPC_40    FPC_41  \\\n",
       "0   -0.041808  0.032636 -0.120359  ...  0.078227 -0.115859  0.002507   \n",
       "1   -0.073673  0.041247 -0.115501  ...  0.038899 -0.087622 -0.001160   \n",
       "2    0.046433  0.031110  0.035902  ... -0.004013  0.042103  0.023665   \n",
       "3    0.036126  0.058780 -0.003189  ... -0.005380  0.051569 -0.000363   \n",
       "4   -0.113544  0.143626  0.046215  ... -0.017159  0.086668 -0.004663   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "365 -0.008893  0.068726 -0.120261  ...  0.026748 -0.079238 -0.099450   \n",
       "366  0.043170 -0.031374  0.052864  ...  0.074753  0.006285 -0.052607   \n",
       "367  0.035902  0.068759  0.169236  ... -0.009194 -0.029963 -0.062132   \n",
       "368  0.039591  0.041182  0.032756  ... -0.054471 -0.025476  0.002272   \n",
       "369  0.070466  0.091987  0.070810  ...  0.000134 -0.058377  0.020101   \n",
       "\n",
       "       FPC_42    FPC_43    FPC_44    FPC_45    FPC_46    FPC_47  Label  \n",
       "0   -0.071014  0.003743 -0.071799 -0.013446  0.023629 -0.033078     CN  \n",
       "1   -0.033449 -0.000071 -0.028571 -0.012131  0.011141 -0.051622     CN  \n",
       "2   -0.045756 -0.038222 -0.026490  0.016583  0.026718  0.021704     CN  \n",
       "3   -0.070958 -0.026215 -0.062745  0.015311  0.015830  0.027684     CN  \n",
       "4   -0.079088 -0.082042 -0.019592  0.076686  0.003082  0.001188     CN  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "365 -0.002123 -0.015750  0.043629 -0.051654 -0.040108  0.055429     CN  \n",
       "366 -0.026298  0.014952  0.046099 -0.042051 -0.056600  0.107450     CN  \n",
       "367 -0.039073 -0.053222  0.018678  0.022814 -0.050268  0.074394     CN  \n",
       "368  0.013597 -0.050911  0.034598  0.041045 -0.016493  0.003273     CN  \n",
       "369  0.021607 -0.031874  0.024500  0.016204  0.007993  0.006334     CN  \n",
       "\n",
       "[370 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temporal_right = fpcas_temporal_right.merge(image_labels, left_on = \"file\", right_on = \"Image\").drop([\"file\", \"Image\"], axis=1)\n",
    "data = df_temporal_right.copy()\n",
    "df_temporal_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos la GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(latent_size, condition_size, output_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(latent_size + condition_size,)))    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dense(64, input_shape=(latent_size + condition_size,)))    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "\n",
    "    model.add(layers.Dense(output_size, activation =\"linear\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(input_size, condition_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(128, input_shape=(input_size + condition_size,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())    \n",
    "    \n",
    "    model.add(layers.Dense(64, input_shape=(input_size + condition_size,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())    \n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funci칩n de p칠rdida y optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, images, labels, noise_dim, epoch):\n",
    "    #print(\"Images shape: \", images.shape, \"Labels shape: \", labels.shape)\n",
    "    noise = tf.random.normal([labels.shape[0], noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        #labels = tf.reshape(labels, (BATCH_SIZE,1))\n",
    "        if epoch % 2 == 0:\n",
    "            disc_train = False\n",
    "        else: \n",
    "            disc_train = True\n",
    "        #disc_train = True\n",
    "        \n",
    "        generated_images = generator(\n",
    "            tf.concat([\n",
    "                noise, \n",
    "                labels],\n",
    "                axis = 1), \n",
    "            training=disc_train)\n",
    "        \n",
    "        \n",
    "        real_output = discriminator(\n",
    "            tf.concat([\n",
    "                images, \n",
    "                labels],\n",
    "                axis = 1), \n",
    "            training=disc_train)\n",
    "        \n",
    "        \n",
    "        fake_output = discriminator(\n",
    "            tf.concat([\n",
    "                generated_images, \n",
    "                labels], \n",
    "                axis = 1), \n",
    "            training=disc_train)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    if epoch % 2 == 0:\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss.numpy(), disc_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, dataset, epochs, noise_dim):\n",
    "    history = []\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "\n",
    "        gen_loss, disc_loss = 0, 0\n",
    "        count = 0\n",
    "        # TODO: pasar imagenes bien\n",
    "        for image_batch, labels in dataset:\n",
    "            g, d = train_step(generator, discriminator, image_batch, labels, noise_dim, epoch)    \n",
    "            gen_loss += g\n",
    "            disc_loss += d\n",
    "            count += 1\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print ('Disc loss: {}, Gen loss: {}. Time for epoch {} is {} sec'.format(disc_loss / count, gen_loss / count, epoch, time.time()-start))\n",
    "            history.append((disc_loss / count, gen_loss / count, epoch))\n",
    "            start = time.time()\n",
    "            \n",
    "    print ('Disc loss: {}, Gen loss: {}. Time for epoch {} is {} sec'.format(disc_loss / count, gen_loss / count, epoch, time.time()-start))\n",
    "    history.append((disc_loss / count, gen_loss / count, epoch))\n",
    "    \n",
    "    return history\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci칩n de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPC_1</th>\n",
       "      <th>FPC_2</th>\n",
       "      <th>FPC_3</th>\n",
       "      <th>FPC_4</th>\n",
       "      <th>FPC_5</th>\n",
       "      <th>FPC_6</th>\n",
       "      <th>FPC_7</th>\n",
       "      <th>FPC_8</th>\n",
       "      <th>FPC_9</th>\n",
       "      <th>FPC_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FPC_39</th>\n",
       "      <th>FPC_40</th>\n",
       "      <th>FPC_41</th>\n",
       "      <th>FPC_42</th>\n",
       "      <th>FPC_43</th>\n",
       "      <th>FPC_44</th>\n",
       "      <th>FPC_45</th>\n",
       "      <th>FPC_46</th>\n",
       "      <th>FPC_47</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.426598</td>\n",
       "      <td>0.112181</td>\n",
       "      <td>-0.322595</td>\n",
       "      <td>-0.033974</td>\n",
       "      <td>0.137037</td>\n",
       "      <td>-0.154825</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>-0.041808</td>\n",
       "      <td>0.032636</td>\n",
       "      <td>-0.120359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078227</td>\n",
       "      <td>-0.115859</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.071014</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>-0.071799</td>\n",
       "      <td>-0.013446</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>-0.033078</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.447385</td>\n",
       "      <td>0.120397</td>\n",
       "      <td>-0.326797</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>0.071308</td>\n",
       "      <td>-0.121966</td>\n",
       "      <td>0.084857</td>\n",
       "      <td>-0.073673</td>\n",
       "      <td>0.041247</td>\n",
       "      <td>-0.115501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038899</td>\n",
       "      <td>-0.087622</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.033449</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>-0.012131</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>-0.051622</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.671375</td>\n",
       "      <td>0.115622</td>\n",
       "      <td>0.097130</td>\n",
       "      <td>0.073061</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.046433</td>\n",
       "      <td>0.031110</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>0.023665</td>\n",
       "      <td>-0.045756</td>\n",
       "      <td>-0.038222</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.635482</td>\n",
       "      <td>0.068551</td>\n",
       "      <td>0.149279</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>-0.014603</td>\n",
       "      <td>0.046951</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.003189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005380</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>-0.070958</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.062745</td>\n",
       "      <td>0.015311</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.531853</td>\n",
       "      <td>-0.371275</td>\n",
       "      <td>0.160936</td>\n",
       "      <td>-0.147053</td>\n",
       "      <td>0.181173</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>-0.113544</td>\n",
       "      <td>0.143626</td>\n",
       "      <td>0.046215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017159</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>-0.004663</td>\n",
       "      <td>-0.079088</td>\n",
       "      <td>-0.082042</td>\n",
       "      <td>-0.019592</td>\n",
       "      <td>0.076686</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1.512948</td>\n",
       "      <td>-0.373168</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>-0.007119</td>\n",
       "      <td>-0.110302</td>\n",
       "      <td>-0.001779</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>-0.120261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>-0.079238</td>\n",
       "      <td>-0.099450</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.015750</td>\n",
       "      <td>0.043629</td>\n",
       "      <td>-0.051654</td>\n",
       "      <td>-0.040108</td>\n",
       "      <td>0.055429</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1.537276</td>\n",
       "      <td>0.165208</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>-0.045060</td>\n",
       "      <td>-0.034294</td>\n",
       "      <td>0.063803</td>\n",
       "      <td>-0.100678</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>-0.031374</td>\n",
       "      <td>0.052864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074753</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>-0.052607</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>-0.056600</td>\n",
       "      <td>0.107450</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1.403889</td>\n",
       "      <td>0.080721</td>\n",
       "      <td>-0.193643</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>-0.048856</td>\n",
       "      <td>0.132803</td>\n",
       "      <td>0.133546</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.169236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009194</td>\n",
       "      <td>-0.029963</td>\n",
       "      <td>-0.062132</td>\n",
       "      <td>-0.039073</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>-0.050268</td>\n",
       "      <td>0.074394</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1.549337</td>\n",
       "      <td>0.191282</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>0.054031</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.025229</td>\n",
       "      <td>0.044374</td>\n",
       "      <td>0.039591</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054471</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>-0.050911</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.041045</td>\n",
       "      <td>-0.016493</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1.536159</td>\n",
       "      <td>0.230381</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>-0.066380</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>-0.008271</td>\n",
       "      <td>-0.019923</td>\n",
       "      <td>0.070466</td>\n",
       "      <td>0.091987</td>\n",
       "      <td>0.070810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>-0.058377</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.021607</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows 칑 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FPC_1     FPC_2     FPC_3     FPC_4     FPC_5     FPC_6     FPC_7  \\\n",
       "0    1.426598  0.112181 -0.322595 -0.033974  0.137037 -0.154825  0.014088   \n",
       "1    1.447385  0.120397 -0.326797 -0.004794  0.071308 -0.121966  0.084857   \n",
       "2    1.671375  0.115622  0.097130  0.073061 -0.007771  0.018225  0.117936   \n",
       "3    1.635482  0.068551  0.149279  0.064166 -0.014603  0.046951  0.025481   \n",
       "4    1.531853 -0.371275  0.160936 -0.147053  0.181173  0.045500  0.081013   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "365  1.512948 -0.373168  0.136220 -0.007119 -0.110302 -0.001779 -0.073120   \n",
       "366  1.537276  0.165208  0.091943 -0.045060 -0.034294  0.063803 -0.100678   \n",
       "367  1.403889  0.080721 -0.193643  0.015488 -0.048856  0.132803  0.133546   \n",
       "368  1.549337  0.191282  0.039588  0.054031  0.033168  0.025229  0.044374   \n",
       "369  1.536159  0.230381  0.087843 -0.066380 -0.010315 -0.008271 -0.019923   \n",
       "\n",
       "        FPC_8     FPC_9    FPC_10  ...    FPC_39    FPC_40    FPC_41  \\\n",
       "0   -0.041808  0.032636 -0.120359  ...  0.078227 -0.115859  0.002507   \n",
       "1   -0.073673  0.041247 -0.115501  ...  0.038899 -0.087622 -0.001160   \n",
       "2    0.046433  0.031110  0.035902  ... -0.004013  0.042103  0.023665   \n",
       "3    0.036126  0.058780 -0.003189  ... -0.005380  0.051569 -0.000363   \n",
       "4   -0.113544  0.143626  0.046215  ... -0.017159  0.086668 -0.004663   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "365 -0.008893  0.068726 -0.120261  ...  0.026748 -0.079238 -0.099450   \n",
       "366  0.043170 -0.031374  0.052864  ...  0.074753  0.006285 -0.052607   \n",
       "367  0.035902  0.068759  0.169236  ... -0.009194 -0.029963 -0.062132   \n",
       "368  0.039591  0.041182  0.032756  ... -0.054471 -0.025476  0.002272   \n",
       "369  0.070466  0.091987  0.070810  ...  0.000134 -0.058377  0.020101   \n",
       "\n",
       "       FPC_42    FPC_43    FPC_44    FPC_45    FPC_46    FPC_47  Label  \n",
       "0   -0.071014  0.003743 -0.071799 -0.013446  0.023629 -0.033078     CN  \n",
       "1   -0.033449 -0.000071 -0.028571 -0.012131  0.011141 -0.051622     CN  \n",
       "2   -0.045756 -0.038222 -0.026490  0.016583  0.026718  0.021704     CN  \n",
       "3   -0.070958 -0.026215 -0.062745  0.015311  0.015830  0.027684     CN  \n",
       "4   -0.079088 -0.082042 -0.019592  0.076686  0.003082  0.001188     CN  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "365 -0.002123 -0.015750  0.043629 -0.051654 -0.040108  0.055429     CN  \n",
       "366 -0.026298  0.014952  0.046099 -0.042051 -0.056600  0.107450     CN  \n",
       "367 -0.039073 -0.053222  0.018678  0.022814 -0.050268  0.074394     CN  \n",
       "368  0.013597 -0.050911  0.034598  0.041045 -0.016493  0.003273     CN  \n",
       "369  0.021607 -0.031874  0.024500  0.016204  0.007993  0.006334     CN  \n",
       "\n",
       "[370 rows x 48 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gan = data.copy()\n",
    "\n",
    "BUFFER_SIZE = data_gan.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCH = 1000\n",
    "YtoX_noise_dim = 50\n",
    "XtoY_noise_dim = 5\n",
    "\n",
    "\n",
    "data_gan = data.sample(frac = 1) # Permutamos el dataset\n",
    "X = data_gan.drop([\"Label\"], axis = 1).values\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis=0)\n",
    "Y = data_gan[\"Label\"].apply(lambda x: 1 if x == \"AD\" else 0).values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datasets Y&rarr;X y X&rarr;Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "YtoX_dataset = tf.data.Dataset.from_tensor_slices((X.astype(\"float32\"), Y.astype(\"float32\"))).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "XtoY_dataset = tf.data.Dataset.from_tensor_slices((Y.astype(\"float32\"), X.astype(\"float32\"))).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Y &rarr; X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "YtoX_generator = make_generator_model(YtoX_noise_dim, Y.shape[1], X.shape[1])\n",
    "YtoX_discriminator = make_discriminator_model(X.shape[1], Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc loss: 1.373312731583913, Gen loss: 0.6498998999595642. Time for epoch 0 is 0.635037899017334 sec\n",
      "Disc loss: 1.3474819660186768, Gen loss: 0.6266331573327383. Time for epoch 10 is 2.151697874069214 sec\n",
      "Disc loss: 1.3043315609296162, Gen loss: 0.6136070787906647. Time for epoch 20 is 2.0976877212524414 sec\n",
      "Disc loss: 1.279041091601054, Gen loss: 0.6138083438078562. Time for epoch 30 is 2.1457042694091797 sec\n",
      "Disc loss: 1.2711676955223083, Gen loss: 0.6347746551036835. Time for epoch 40 is 2.129661798477173 sec\n",
      "Disc loss: 1.3026732405026753, Gen loss: 0.6555717388788859. Time for epoch 50 is 2.0520212650299072 sec\n",
      "Disc loss: 1.3163567980130513, Gen loss: 0.6952225267887115. Time for epoch 60 is 2.047999858856201 sec\n",
      "Disc loss: 1.4241509040196736, Gen loss: 0.6244474252065023. Time for epoch 70 is 2.057943105697632 sec\n",
      "Disc loss: 1.438522179921468, Gen loss: 0.6175770362218221. Time for epoch 80 is 2.128282070159912 sec\n",
      "Disc loss: 1.3435481389363606, Gen loss: 0.6797130604585012. Time for epoch 90 is 2.098874568939209 sec\n",
      "Disc loss: 1.3381855289141338, Gen loss: 0.679741362730662. Time for epoch 100 is 2.0497689247131348 sec\n",
      "Disc loss: 1.4136323928833008, Gen loss: 0.6555480659008026. Time for epoch 110 is 2.088027238845825 sec\n",
      "Disc loss: 1.350720504919688, Gen loss: 0.732883075873057. Time for epoch 120 is 2.206433057785034 sec\n",
      "Disc loss: 1.3243739406267803, Gen loss: 0.7174886663754781. Time for epoch 130 is 2.2095589637756348 sec\n",
      "Disc loss: 1.3481755654017131, Gen loss: 0.6918732623259226. Time for epoch 140 is 2.208202838897705 sec\n",
      "Disc loss: 1.2818338672320049, Gen loss: 0.7362993756930033. Time for epoch 150 is 2.2369766235351562 sec\n",
      "Disc loss: 1.1930734515190125, Gen loss: 0.8257322708765665. Time for epoch 160 is 2.1842241287231445 sec\n",
      "Disc loss: 1.2747238278388977, Gen loss: 0.7581079800923666. Time for epoch 170 is 2.1687722206115723 sec\n",
      "Disc loss: 1.4113823374112446, Gen loss: 0.6825146079063416. Time for epoch 180 is 2.3270089626312256 sec\n",
      "Disc loss: 1.3707706133524578, Gen loss: 0.7380350232124329. Time for epoch 190 is 2.119802951812744 sec\n",
      "Disc loss: 1.3067182302474976, Gen loss: 0.75518599152565. Time for epoch 200 is 2.1569995880126953 sec\n",
      "Disc loss: 1.2838294704755147, Gen loss: 0.7500945826371511. Time for epoch 210 is 2.0869240760803223 sec\n",
      "Disc loss: 1.2107394337654114, Gen loss: 0.8153866132100424. Time for epoch 220 is 2.12009596824646 sec\n",
      "Disc loss: 1.252767542997996, Gen loss: 0.7642253041267395. Time for epoch 230 is 2.1250221729278564 sec\n",
      "Disc loss: 1.3667230208714802, Gen loss: 0.6852909525235494. Time for epoch 240 is 2.107746124267578 sec\n",
      "Disc loss: 1.3079060117403667, Gen loss: 0.750105490287145. Time for epoch 250 is 2.104008913040161 sec\n",
      "Disc loss: 1.1669683456420898, Gen loss: 0.8600466549396515. Time for epoch 260 is 2.0905635356903076 sec\n",
      "Disc loss: 1.2636996507644653, Gen loss: 0.7452989717324575. Time for epoch 270 is 2.092244863510132 sec\n",
      "Disc loss: 1.3043896754582722, Gen loss: 0.7475631932417551. Time for epoch 280 is 2.110316514968872 sec\n",
      "Disc loss: 1.2586360772450764, Gen loss: 0.8249703347682953. Time for epoch 290 is 2.0669994354248047 sec\n",
      "Disc loss: 1.3061948418617249, Gen loss: 0.7624164323012034. Time for epoch 300 is 2.080583333969116 sec\n",
      "Disc loss: 1.2463055650393169, Gen loss: 0.8257030546665192. Time for epoch 310 is 2.1032142639160156 sec\n",
      "Disc loss: 1.2846306959788005, Gen loss: 0.7688624958197275. Time for epoch 320 is 2.061262845993042 sec\n",
      "Disc loss: 1.2837888598442078, Gen loss: 0.7857535382111868. Time for epoch 330 is 2.0460801124572754 sec\n",
      "Disc loss: 1.204513708750407, Gen loss: 0.8579227526982626. Time for epoch 340 is 2.028996706008911 sec\n",
      "Disc loss: 1.228616178035736, Gen loss: 0.8308067818482717. Time for epoch 350 is 2.0108962059020996 sec\n",
      "Disc loss: 1.2159922520319622, Gen loss: 0.8918518523375193. Time for epoch 360 is 2.0600547790527344 sec\n",
      "Disc loss: 1.3268093665440877, Gen loss: 0.7450974881649017. Time for epoch 370 is 2.0680429935455322 sec\n",
      "Disc loss: 1.2118332783381145, Gen loss: 0.8199878036975861. Time for epoch 380 is 2.031771183013916 sec\n",
      "Disc loss: 1.0885552763938904, Gen loss: 0.930306871732076. Time for epoch 390 is 2.0130321979522705 sec\n",
      "Disc loss: 1.210340718428294, Gen loss: 0.8083818554878235. Time for epoch 400 is 2.0289995670318604 sec\n",
      "Disc loss: 1.2647901773452759, Gen loss: 0.8341955641905466. Time for epoch 410 is 2.052143096923828 sec\n",
      "Disc loss: 1.2625335653622944, Gen loss: 0.8414648572603861. Time for epoch 420 is 2.0691428184509277 sec\n",
      "Disc loss: 1.2239060997962952, Gen loss: 0.872398167848587. Time for epoch 430 is 2.0879883766174316 sec\n",
      "Disc loss: 1.1670563220977783, Gen loss: 0.9139622350533804. Time for epoch 440 is 2.0490124225616455 sec\n",
      "Disc loss: 1.1964077552159627, Gen loss: 0.8354650636514028. Time for epoch 450 is 2.0620293617248535 sec\n",
      "Disc loss: 1.2474021514256795, Gen loss: 0.7946419417858124. Time for epoch 460 is 2.0920722484588623 sec\n",
      "Disc loss: 1.2555689414342244, Gen loss: 0.8089964985847473. Time for epoch 470 is 2.1110448837280273 sec\n",
      "Disc loss: 1.1729445656140645, Gen loss: 0.8872182766596476. Time for epoch 480 is 2.0780081748962402 sec\n",
      "Disc loss: 1.1683849294980366, Gen loss: 0.8593941728274027. Time for epoch 490 is 2.078627824783325 sec\n",
      "Disc loss: 1.2185721794764202, Gen loss: 0.8036752641201019. Time for epoch 500 is 2.1127138137817383 sec\n",
      "Disc loss: 1.2121700843175252, Gen loss: 0.8056858281294504. Time for epoch 510 is 2.133605718612671 sec\n",
      "Disc loss: 1.1407378514607747, Gen loss: 0.8927335739135742. Time for epoch 520 is 2.121781587600708 sec\n",
      "Disc loss: 1.1540780067443848, Gen loss: 0.881096214056015. Time for epoch 530 is 2.081111431121826 sec\n",
      "Disc loss: 1.2072242895762126, Gen loss: 0.8552610774834951. Time for epoch 540 is 2.070054054260254 sec\n",
      "Disc loss: 1.1824345588684082, Gen loss: 0.9071911871433258. Time for epoch 550 is 2.1648800373077393 sec\n",
      "Disc loss: 1.209049681822459, Gen loss: 0.8796002070109049. Time for epoch 560 is 2.1955535411834717 sec\n",
      "Disc loss: 1.2035608291625977, Gen loss: 0.8750191430250803. Time for epoch 570 is 2.1146557331085205 sec\n",
      "Disc loss: 1.230727990468343, Gen loss: 0.8133157094319662. Time for epoch 580 is 2.085524320602417 sec\n",
      "Disc loss: 1.2362202207247417, Gen loss: 0.780967245499293. Time for epoch 590 is 2.0949995517730713 sec\n",
      "Disc loss: 1.1694615681966145, Gen loss: 0.8291592498620352. Time for epoch 600 is 2.1422441005706787 sec\n",
      "Disc loss: 1.2082528074582417, Gen loss: 0.7924203177293142. Time for epoch 610 is 2.1400256156921387 sec\n",
      "Disc loss: 1.2336649696032207, Gen loss: 0.8153676688671112. Time for epoch 620 is 2.085632801055908 sec\n",
      "Disc loss: 1.1654358506202698, Gen loss: 0.9205103615919749. Time for epoch 630 is 2.0335137844085693 sec\n",
      "Disc loss: 1.2737234433492024, Gen loss: 0.8278984030087789. Time for epoch 640 is 2.042785882949829 sec\n",
      "Disc loss: 1.1972549756368, Gen loss: 0.9569649795691172. Time for epoch 650 is 2.0769996643066406 sec\n",
      "Disc loss: 1.1775610248247783, Gen loss: 0.8731347620487213. Time for epoch 660 is 2.076026201248169 sec\n",
      "Disc loss: 1.2436261375745137, Gen loss: 0.7980069220066071. Time for epoch 670 is 2.023000478744507 sec\n",
      "Disc loss: 1.184169868628184, Gen loss: 0.8767881790796915. Time for epoch 680 is 2.029163122177124 sec\n",
      "Disc loss: 1.2657817006111145, Gen loss: 0.7836641271909078. Time for epoch 690 is 2.056999921798706 sec\n",
      "Disc loss: 1.2709840933481853, Gen loss: 0.8003380596637726. Time for epoch 700 is 2.07218074798584 sec\n",
      "Disc loss: 1.1732561389605205, Gen loss: 0.8764299949010214. Time for epoch 710 is 2.061635971069336 sec\n",
      "Disc loss: 1.1973799268404643, Gen loss: 0.8202981154123942. Time for epoch 720 is 2.023029088973999 sec\n",
      "Disc loss: 1.2240827679634094, Gen loss: 0.8247624138991038. Time for epoch 730 is 2.0275955200195312 sec\n",
      "Disc loss: 1.2201991081237793, Gen loss: 0.8282488286495209. Time for epoch 740 is 2.0670011043548584 sec\n",
      "Disc loss: 1.2218217452367146, Gen loss: 0.8455331524213155. Time for epoch 750 is 2.1370017528533936 sec\n",
      "Disc loss: 1.1999707023302715, Gen loss: 0.8855750958124796. Time for epoch 760 is 2.0911996364593506 sec\n",
      "Disc loss: 1.206263820330302, Gen loss: 0.8522751132647196. Time for epoch 770 is 2.0659992694854736 sec\n",
      "Disc loss: 1.2024810512860615, Gen loss: 0.8353627522786459. Time for epoch 780 is 2.078000783920288 sec\n",
      "Disc loss: 1.1848944226900737, Gen loss: 0.8350088099638621. Time for epoch 790 is 2.114002227783203 sec\n",
      "Disc loss: 1.2355501055717468, Gen loss: 0.7826172610123953. Time for epoch 800 is 2.1242072582244873 sec\n",
      "Disc loss: 1.2353925704956055, Gen loss: 0.8136214514573415. Time for epoch 810 is 2.104001045227051 sec\n",
      "Disc loss: 1.1530220707257588, Gen loss: 0.9212762812773386. Time for epoch 820 is 2.1219987869262695 sec\n",
      "Disc loss: 1.2241182525952656, Gen loss: 0.8165969848632812. Time for epoch 830 is 2.090672731399536 sec\n",
      "Disc loss: 1.2416014870007832, Gen loss: 0.8329471945762634. Time for epoch 840 is 2.0963664054870605 sec\n",
      "Disc loss: 1.1432795127232869, Gen loss: 0.9292720357577006. Time for epoch 850 is 2.147921085357666 sec\n",
      "Disc loss: 1.1849522590637207, Gen loss: 0.8512145380179087. Time for epoch 860 is 2.0813393592834473 sec\n",
      "Disc loss: 1.2101399302482605, Gen loss: 0.8615540166695913. Time for epoch 870 is 2.1069228649139404 sec\n",
      "Disc loss: 1.1589094201723735, Gen loss: 0.8899839123090109. Time for epoch 880 is 2.1266372203826904 sec\n",
      "Disc loss: 1.1999518076578777, Gen loss: 0.830514391263326. Time for epoch 890 is 2.148996114730835 sec\n",
      "Disc loss: 1.2325705289840698, Gen loss: 0.8008394837379456. Time for epoch 900 is 2.1315419673919678 sec\n",
      "Disc loss: 1.1361644466718037, Gen loss: 0.8794366220633189. Time for epoch 910 is 2.086022138595581 sec\n",
      "Disc loss: 1.1195245583852131, Gen loss: 0.888079841931661. Time for epoch 920 is 2.1665167808532715 sec\n",
      "Disc loss: 1.1907466252644856, Gen loss: 0.8399976392587026. Time for epoch 930 is 2.136887550354004 sec\n",
      "Disc loss: 1.1536891857783, Gen loss: 0.899387131134669. Time for epoch 940 is 2.0934698581695557 sec\n",
      "Disc loss: 1.1240939100583394, Gen loss: 0.9020819167296091. Time for epoch 950 is 2.076519727706909 sec\n",
      "Disc loss: 1.1708719333012898, Gen loss: 0.833295742670695. Time for epoch 960 is 2.0789990425109863 sec\n",
      "Disc loss: 1.1439773639043171, Gen loss: 0.8810968200365702. Time for epoch 970 is 2.090782642364502 sec\n",
      "Disc loss: 1.155383586883545, Gen loss: 0.8576788504918417. Time for epoch 980 is 2.0986835956573486 sec\n",
      "Disc loss: 1.2399753332138062, Gen loss: 0.7968646784623464. Time for epoch 990 is 2.0844578742980957 sec\n",
      "Disc loss: 1.3703296979268391, Gen loss: 0.804424911737442. Time for epoch 999 is 1.891059160232544 sec\n"
     ]
    }
   ],
   "source": [
    "YtoX_history = train(YtoX_generator, YtoX_discriminator, YtoX_dataset, N_EPOCH, YtoX_noise_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN X &rarr; Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtoY_generator = make_generator_model(XtoY_noise_dim, X.shape[1], Y.shape[1])\n",
    "XtoY_discriminator = make_discriminator_model(Y.shape[1], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc loss: 1.483480970064799, Gen loss: 0.6355150640010834. Time for epoch 0 is 0.19099998474121094 sec\n",
      "Disc loss: 1.8256706794102986, Gen loss: 0.46653980016708374. Time for epoch 10 is 2.034658432006836 sec\n",
      "Disc loss: 1.0755986173947651, Gen loss: 1.0682805279890697. Time for epoch 20 is 2.052999973297119 sec\n",
      "Disc loss: 1.969055414199829, Gen loss: 0.4563161830107371. Time for epoch 30 is 2.056826114654541 sec\n",
      "Disc loss: 1.0833531618118286, Gen loss: 0.9909741878509521. Time for epoch 40 is 2.0725321769714355 sec\n",
      "Disc loss: 1.7565178473790486, Gen loss: 0.5118977626164755. Time for epoch 50 is 2.0060019493103027 sec\n",
      "Disc loss: 1.2651514808336894, Gen loss: 0.9078514476617178. Time for epoch 60 is 2.0769081115722656 sec\n",
      "Disc loss: 1.5009206533432007, Gen loss: 0.6101399461428324. Time for epoch 70 is 2.094919204711914 sec\n",
      "Disc loss: 1.3542619744936626, Gen loss: 0.7377816239992777. Time for epoch 80 is 2.107999086380005 sec\n",
      "Disc loss: 1.4185601075490315, Gen loss: 0.6709955235322317. Time for epoch 90 is 2.082319736480713 sec\n",
      "Disc loss: 1.3829586108525593, Gen loss: 0.7166170477867126. Time for epoch 100 is 2.0711240768432617 sec\n",
      "Disc loss: 1.3870215614636738, Gen loss: 0.697121798992157. Time for epoch 110 is 2.0835628509521484 sec\n",
      "Disc loss: 1.3864038387934368, Gen loss: 0.700766901175181. Time for epoch 120 is 2.1163296699523926 sec\n",
      "Disc loss: 1.3846547206242878, Gen loss: 0.7002321680386862. Time for epoch 130 is 2.1175146102905273 sec\n",
      "Disc loss: 1.3824734886487324, Gen loss: 0.7031085391839346. Time for epoch 140 is 2.093920946121216 sec\n",
      "Disc loss: 1.3842070698738098, Gen loss: 0.700060615936915. Time for epoch 150 is 2.0721254348754883 sec\n",
      "Disc loss: 1.3831952412923176, Gen loss: 0.6989205280939738. Time for epoch 160 is 2.0897722244262695 sec\n",
      "Disc loss: 1.3824251890182495, Gen loss: 0.700252890586853. Time for epoch 170 is 2.116997480392456 sec\n",
      "Disc loss: 1.3826150099436443, Gen loss: 0.6995590329170227. Time for epoch 180 is 2.09678053855896 sec\n",
      "Disc loss: 1.3834884365399678, Gen loss: 0.6965036193529764. Time for epoch 190 is 2.05521559715271 sec\n",
      "Disc loss: 1.3828559517860413, Gen loss: 0.6975438495477041. Time for epoch 200 is 2.0689961910247803 sec\n",
      "Disc loss: 1.3824080228805542, Gen loss: 0.697045256694158. Time for epoch 210 is 2.1030030250549316 sec\n",
      "Disc loss: 1.3813893000284831, Gen loss: 0.698731412490209. Time for epoch 220 is 2.1977412700653076 sec\n",
      "Disc loss: 1.3823935588200886, Gen loss: 0.6980876723925272. Time for epoch 230 is 2.089827537536621 sec\n",
      "Disc loss: 1.3827061255772908, Gen loss: 0.6975705027580261. Time for epoch 240 is 1.9253671169281006 sec\n",
      "Disc loss: 1.3820224404335022, Gen loss: 0.6983464558919271. Time for epoch 250 is 1.8784387111663818 sec\n",
      "Disc loss: 1.3818540573120117, Gen loss: 0.6988324423631033. Time for epoch 260 is 1.8900096416473389 sec\n",
      "Disc loss: 1.3822633028030396, Gen loss: 0.6984511613845825. Time for epoch 270 is 1.8990318775177002 sec\n",
      "Disc loss: 1.3820929725964863, Gen loss: 0.6984664698441824. Time for epoch 280 is 1.904721736907959 sec\n",
      "Disc loss: 1.3815560539563496, Gen loss: 0.6992548306783041. Time for epoch 290 is 1.8550000190734863 sec\n",
      "Disc loss: 1.382327675819397, Gen loss: 0.6961408654848734. Time for epoch 300 is 1.866999864578247 sec\n",
      "Disc loss: 1.3825310667355855, Gen loss: 0.6972639163335165. Time for epoch 310 is 1.8760004043579102 sec\n",
      "Disc loss: 1.3813701073328655, Gen loss: 0.6975542108217875. Time for epoch 320 is 1.9040029048919678 sec\n",
      "Disc loss: 1.3829931020736694, Gen loss: 0.6957123279571533. Time for epoch 330 is 1.9599981307983398 sec\n",
      "Disc loss: 1.3825027346611023, Gen loss: 0.6970032950242361. Time for epoch 340 is 1.8610005378723145 sec\n",
      "Disc loss: 1.3827748894691467, Gen loss: 0.6961096624533335. Time for epoch 350 is 1.8569984436035156 sec\n",
      "Disc loss: 1.382355531056722, Gen loss: 0.6963872214158376. Time for epoch 360 is 1.875000238418579 sec\n",
      "Disc loss: 1.3824311097462971, Gen loss: 0.6964654624462128. Time for epoch 370 is 1.8919999599456787 sec\n",
      "Disc loss: 1.3822150627772014, Gen loss: 0.6965493957201639. Time for epoch 380 is 1.905000925064087 sec\n",
      "Disc loss: 1.382316569487254, Gen loss: 0.696203609307607. Time for epoch 390 is 1.9209990501403809 sec\n",
      "Disc loss: 1.3829550941785176, Gen loss: 0.6967118084430695. Time for epoch 400 is 1.9320001602172852 sec\n",
      "Disc loss: 1.3826600710550945, Gen loss: 0.6981476843357086. Time for epoch 410 is 1.9030003547668457 sec\n",
      "Disc loss: 1.3833803137143452, Gen loss: 0.6951272288958231. Time for epoch 420 is 1.9499995708465576 sec\n",
      "Disc loss: 1.3835598230361938, Gen loss: 0.6956222554047903. Time for epoch 430 is 1.9210000038146973 sec\n",
      "Disc loss: 1.3828647136688232, Gen loss: 0.6953751941521963. Time for epoch 440 is 1.9190011024475098 sec\n",
      "Disc loss: 1.3834152619043987, Gen loss: 0.6947746276855469. Time for epoch 450 is 1.9117274284362793 sec\n",
      "Disc loss: 1.382107396920522, Gen loss: 0.6963976720968882. Time for epoch 460 is 1.92799973487854 sec\n",
      "Disc loss: 1.3829426765441895, Gen loss: 0.6948783099651337. Time for epoch 470 is 1.9525911808013916 sec\n",
      "Disc loss: 1.3834574421246846, Gen loss: 0.6962182223796844. Time for epoch 480 is 1.9570000171661377 sec\n",
      "Disc loss: 1.3827492396036785, Gen loss: 0.6963333884874979. Time for epoch 490 is 1.929999828338623 sec\n",
      "Disc loss: 1.3829778830210369, Gen loss: 0.6961645980676016. Time for epoch 500 is 1.9149997234344482 sec\n",
      "Disc loss: 1.3832043210665386, Gen loss: 0.695047934850057. Time for epoch 510 is 1.9275245666503906 sec\n",
      "Disc loss: 1.3833604454994202, Gen loss: 0.6944166719913483. Time for epoch 520 is 1.9570116996765137 sec\n",
      "Disc loss: 1.3831210732460022, Gen loss: 0.6957814693450928. Time for epoch 530 is 2.026003122329712 sec\n",
      "Disc loss: 1.382814089457194, Gen loss: 0.6952783366044363. Time for epoch 540 is 1.9350111484527588 sec\n",
      "Disc loss: 1.3827719688415527, Gen loss: 0.6960350473721822. Time for epoch 550 is 1.940000057220459 sec\n",
      "Disc loss: 1.3829943935076396, Gen loss: 0.6945123672485352. Time for epoch 560 is 1.8809998035430908 sec\n",
      "Disc loss: 1.382919470469157, Gen loss: 0.6944777965545654. Time for epoch 570 is 1.9110002517700195 sec\n",
      "Disc loss: 1.383528192838033, Gen loss: 0.6959308783213297. Time for epoch 580 is 1.8959999084472656 sec\n",
      "Disc loss: 1.3826381762822468, Gen loss: 0.6962436338265737. Time for epoch 590 is 1.862999677658081 sec\n",
      "Disc loss: 1.3834407329559326, Gen loss: 0.693882942199707. Time for epoch 600 is 1.8420004844665527 sec\n",
      "Disc loss: 1.383291224638621, Gen loss: 0.6953601141770681. Time for epoch 610 is 1.895989179611206 sec\n",
      "Disc loss: 1.3834002017974854, Gen loss: 0.6948498984177908. Time for epoch 620 is 2.104001760482788 sec\n",
      "Disc loss: 1.3831171194712322, Gen loss: 0.6950885752836863. Time for epoch 630 is 2.0619986057281494 sec\n",
      "Disc loss: 1.3828335603078206, Gen loss: 0.6963784197966257. Time for epoch 640 is 2.0390148162841797 sec\n",
      "Disc loss: 1.383552094300588, Gen loss: 0.6942375799020132. Time for epoch 650 is 2.1447737216949463 sec\n",
      "Disc loss: 1.3836039900779724, Gen loss: 0.6951672931512197. Time for epoch 660 is 2.238149642944336 sec\n",
      "Disc loss: 1.3830015261967976, Gen loss: 0.6955292721589407. Time for epoch 670 is 2.148603916168213 sec\n",
      "Disc loss: 1.3830724358558655, Gen loss: 0.6944762170314789. Time for epoch 680 is 2.143030881881714 sec\n",
      "Disc loss: 1.383376936117808, Gen loss: 0.6956405738989512. Time for epoch 690 is 2.1328647136688232 sec\n",
      "Disc loss: 1.3830255071322124, Gen loss: 0.6956071158250173. Time for epoch 700 is 2.192176342010498 sec\n",
      "Disc loss: 1.3832418123881023, Gen loss: 0.6963271697362264. Time for epoch 710 is 2.1486282348632812 sec\n",
      "Disc loss: 1.3833781878153484, Gen loss: 0.6942226787408193. Time for epoch 720 is 2.0010037422180176 sec\n",
      "Disc loss: 1.3833130995432537, Gen loss: 0.6963484982649485. Time for epoch 730 is 2.027538776397705 sec\n",
      "Disc loss: 1.3837544322013855, Gen loss: 0.6958653628826141. Time for epoch 740 is 1.9795372486114502 sec\n",
      "Disc loss: 1.3833651145299275, Gen loss: 0.6951662798722585. Time for epoch 750 is 2.0120105743408203 sec\n",
      "Disc loss: 1.383537272612254, Gen loss: 0.6941949427127838. Time for epoch 760 is 1.936000108718872 sec\n",
      "Disc loss: 1.3836529453595479, Gen loss: 0.6954257488250732. Time for epoch 770 is 1.9879999160766602 sec\n",
      "Disc loss: 1.3830353418986003, Gen loss: 0.6956084072589874. Time for epoch 780 is 1.91502046585083 sec\n",
      "Disc loss: 1.3832439581553142, Gen loss: 0.6961048642794291. Time for epoch 790 is 1.9560117721557617 sec\n",
      "Disc loss: 1.383166233698527, Gen loss: 0.6952279508113861. Time for epoch 800 is 2.04298996925354 sec\n",
      "Disc loss: 1.3834154605865479, Gen loss: 0.6934236586093903. Time for epoch 810 is 2.070033073425293 sec\n",
      "Disc loss: 1.3836411635080974, Gen loss: 0.694841593503952. Time for epoch 820 is 2.0832972526550293 sec\n",
      "Disc loss: 1.3833527366320293, Gen loss: 0.695330798625946. Time for epoch 830 is 2.037405252456665 sec\n",
      "Disc loss: 1.383285899957021, Gen loss: 0.6939261853694916. Time for epoch 840 is 2.036320924758911 sec\n",
      "Disc loss: 1.3837327361106873, Gen loss: 0.6949241757392883. Time for epoch 850 is 2.0480570793151855 sec\n",
      "Disc loss: 1.3835582931836445, Gen loss: 0.6949957609176636. Time for epoch 860 is 2.161996841430664 sec\n",
      "Disc loss: 1.383323609828949, Gen loss: 0.696122278769811. Time for epoch 870 is 1.9405441284179688 sec\n",
      "Disc loss: 1.3834213614463806, Gen loss: 0.6923165420691172. Time for epoch 880 is 1.959989309310913 sec\n",
      "Disc loss: 1.3832257986068726, Gen loss: 0.6951945026715597. Time for epoch 890 is 2.0354397296905518 sec\n",
      "Disc loss: 1.3836241761843364, Gen loss: 0.6938462654749552. Time for epoch 900 is 2.0101356506347656 sec\n",
      "Disc loss: 1.3835973342259724, Gen loss: 0.6941482623418173. Time for epoch 910 is 2.0180091857910156 sec\n",
      "Disc loss: 1.3837157487869263, Gen loss: 0.6947433551152548. Time for epoch 920 is 2.034045457839966 sec\n",
      "Disc loss: 1.3836975693702698, Gen loss: 0.6947639286518097. Time for epoch 930 is 1.9920156002044678 sec\n",
      "Disc loss: 1.3837503393491108, Gen loss: 0.6950889329115549. Time for epoch 940 is 2.0219507217407227 sec\n",
      "Disc loss: 1.3837860027949016, Gen loss: 0.695752869049708. Time for epoch 950 is 2.147001266479492 sec\n",
      "Disc loss: 1.3833416899045308, Gen loss: 0.6949928204218546. Time for epoch 960 is 2.016892671585083 sec\n",
      "Disc loss: 1.3835683862368267, Gen loss: 0.6947899758815765. Time for epoch 970 is 2.025994062423706 sec\n",
      "Disc loss: 1.383219043413798, Gen loss: 0.6945011615753174. Time for epoch 980 is 1.988001823425293 sec\n",
      "Disc loss: 1.3830016454060872, Gen loss: 0.6945454875628153. Time for epoch 990 is 1.9873018264770508 sec\n",
      "Disc loss: 1.3906450271606445, Gen loss: 0.7073932488759359. Time for epoch 999 is 1.9025678634643555 sec\n"
     ]
    }
   ],
   "source": [
    "XtoY_history = train(XtoY_generator, XtoY_discriminator, XtoY_dataset, N_EPOCH, XtoY_noise_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_PROP = 0.7\n",
    "\n",
    "Y_real = Y\n",
    "X_real = X\n",
    "\n",
    "real_dataset = np.concatenate((X_real, Y_real), axis = 1).astype(\"float32\")\n",
    "real_labels = np.ones((real_dataset.shape[0],1), dtype = \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 47)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(generator, num_imgs, noise_dim, condition):\n",
    "    noise = tf.random.normal([num_imgs, noise_dim], dtype= \"float32\")\n",
    "    labels = tf.convert_to_tensor(condition, dtype= \"float32\")\n",
    "    inp = tf.concat([noise, labels], axis = 1)\n",
    "    generated_image = generator(inp, training=False)\n",
    "    return generated_image.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2ST Y &rarr; X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = Y_real.shape[0]\n",
    "\n",
    "X_fake = generate_examples(YtoX_generator, n_images, YtoX_noise_dim, Y_real)\n",
    "YtoX_fake_dataset = np.concatenate((X_fake, Y_real), axis = 1).astype(\"float32\")\n",
    "YtoX_fake_labels = np.zeros((n_images,1), dtype= \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "YtoX_total_dataset = np.concatenate((real_dataset, YtoX_fake_dataset), axis = 0)\n",
    "YtoX_total_labels = np.concatenate((real_labels, YtoX_fake_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = YtoX_total_dataset.shape[0]\n",
    "\n",
    "random_range = np.random.permutation(n)\n",
    "\n",
    "train_indexes = random_range[:int(TRAIN_TEST_PROP * n)]\n",
    "n_train = train_indexes.shape[0]\n",
    "test_indexes = random_range[int(TRAIN_TEST_PROP * n):]\n",
    "n_test = test_indexes.shape[0]\n",
    "\n",
    "X_train_YtoX = YtoX_total_dataset[train_indexes, :]\n",
    "Y_train_YtoX = YtoX_total_labels[train_indexes, :].reshape(-1)\n",
    "\n",
    "X_test_YtoX = YtoX_total_dataset[test_indexes, :]\n",
    "Y_test_YtoX = YtoX_total_labels[test_indexes, :].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7792792792792793"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_YtoX = KNeighborsClassifier(n_neighbors=int(math.sqrt(n_train)))\n",
    "# Entrenamos con los datos de entrenamiento\n",
    "classifier_YtoX.fit(X_train_YtoX, Y_train_YtoX)\n",
    "# Evaluamos con los de test\n",
    "accuracy_score(Y_test_YtoX, classifier_YtoX.predict(X_test_YtoX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2ST X &rarr; Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = X_real.shape[0]\n",
    "\n",
    "Y_fake = generate_examples(XtoY_generator, n_images, XtoY_noise_dim, X_real)\n",
    "XtoY_fake_dataset = np.concatenate((X_real, Y_fake), axis = 1).astype(\"float32\")\n",
    "XtoY_fake_labels = np.zeros((n_images,1), dtype= \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtoY_total_dataset = np.concatenate((real_dataset, XtoY_fake_dataset), axis = 0)\n",
    "XtoY_total_labels = np.concatenate((real_labels, XtoY_fake_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = XtoY_total_dataset.shape[0]\n",
    "random_range = np.random.permutation(n)\n",
    "\n",
    "train_indexes = random_range[:int(TRAIN_TEST_PROP * n)]\n",
    "n_train = train_indexes.shape[0]\n",
    "test_indexes = random_range[int(TRAIN_TEST_PROP * n):]\n",
    "n_test = test_indexes.shape[0]\n",
    "\n",
    "X_train_XtoY = XtoY_total_dataset[train_indexes, :]\n",
    "Y_train_XtoY = XtoY_total_labels[train_indexes, :].reshape(-1)\n",
    "\n",
    "X_test_XtoY = XtoY_total_dataset[test_indexes, :]\n",
    "Y_test_XtoY = XtoY_total_labels[test_indexes, :].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40540540540540543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_XtoY = KNeighborsClassifier(n_neighbors=int(math.sqrt(n_train)))\n",
    "# Entrenamos con los datos de entrenamiento\n",
    "classifier_XtoY.fit(X_train_XtoY, Y_train_XtoY)\n",
    "# Evaluamos con los de test\n",
    "accuracy_score(Y_test_XtoY, classifier_XtoY.predict(X_test_XtoY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contraste de hipotesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
